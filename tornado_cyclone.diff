diff --git a/README.md b/README.md
index 0d7c426..272fb14 100644
--- a/README.md
+++ b/README.md
@@ -2,14 +2,14 @@ RedisLive
 ---------
 
 Visualize your redis instances, analyze query patterns and spikes.
+This is a port of the original Redis Line to [cyclone](http://cyclone.io) by [gleicon](http://7co.cc)
 
 ![Redis Live](https://github.com/kumarnitin/RedisLive/blob/master/design/redis-live.png?raw=true "Redis Live")
 
 Installation
 ------------
 **Install Dependencies**
-+ [tornado](https://github.com/facebook/tornado) `pip install tornado`
-+ [redis.py] (https://github.com/andymccurdy/redis-py) `pip install redis`
++ [cyclone](https://cyclone.io) `pip install cyclone`
 + [python-dateutil] (http://labix.org/python-dateutil) `pip install python-dateutil`
 
 You'll also need argparse if you're running Python < 2.7:
diff --git a/src/api/controller/BaseController.py b/src/api/controller/BaseController.py
index 19cad57..914ad27 100644
--- a/src/api/controller/BaseController.py
+++ b/src/api/controller/BaseController.py
@@ -1,12 +1,11 @@
 from dataprovider.dataprovider import RedisLiveDataProvider
-import tornado.ioloop
-import tornado.web
+import cyclone.web
 import dateutil.parser
+from twisted.python import log
 
+class BaseController(cyclone.web.RequestHandler):
 
-class BaseController(tornado.web.RequestHandler):
-
-    stats_provider = RedisLiveDataProvider.get_provider()
+    stats_provider = RedisLiveDataProvider.get_provider('txredis')
 
     def datetime_to_list(self, datetime):
         """Converts a datetime to a list.
diff --git a/src/api/controller/BaseStaticFileHandler.py b/src/api/controller/BaseStaticFileHandler.py
index 162fa62..6343a53 100644
--- a/src/api/controller/BaseStaticFileHandler.py
+++ b/src/api/controller/BaseStaticFileHandler.py
@@ -1,6 +1,6 @@
-import tornado.web
+import cyclone.web
 
-class BaseStaticFileHandler(tornado.web.StaticFileHandler):
+class BaseStaticFileHandler(cyclone.web.StaticFileHandler):
 	def compute_etag(self):
 		return None
 
diff --git a/src/api/controller/CommandsController.py b/src/api/controller/CommandsController.py
index cd9df26..ac046e3 100644
--- a/src/api/controller/CommandsController.py
+++ b/src/api/controller/CommandsController.py
@@ -1,12 +1,12 @@
 from BaseController import BaseController
-import tornado.ioloop
-import tornado.web
 import dateutil.parser
 from datetime import datetime, timedelta
+from twisted.internet import defer
 
 
 class CommandsController(BaseController):
 
+    @defer.inlineCallbacks
     def get(self):
         """Serves a GET request.
         """
@@ -45,7 +45,7 @@ class CommandsController(BaseController):
           group_by = "second"
 
         combined_data = []
-        stats = self.stats_provider.get_command_stats(server, start, end,
+        stats = yield self.stats_provider.get_command_stats(server, start, end,
                                                       group_by)
         for data in stats:
             combined_data.append([data[1], data[0]])
diff --git a/src/api/controller/InfoController.py b/src/api/controller/InfoController.py
index 990b09d..54bc29b 100644
--- a/src/api/controller/InfoController.py
+++ b/src/api/controller/InfoController.py
@@ -1,16 +1,18 @@
 from decimal import Decimal
 from BaseController import BaseController
-import tornado.ioloop
-import tornado.web
 import re
 
+from twisted.internet import defer
+
+
 
 class InfoController(BaseController):
+    @defer.inlineCallbacks
     def get(self):
         """Serves a GET request.
         """
         server = self.get_argument("server")
-        redis_info = self.stats_provider.get_info(server)
+        redis_info = yield self.stats_provider.get_info(server)
         databases=[]
 
         for key in sorted(redis_info.keys()):
diff --git a/src/api/controller/MemoryController.py b/src/api/controller/MemoryController.py
index 1855881..c819f04 100644
--- a/src/api/controller/MemoryController.py
+++ b/src/api/controller/MemoryController.py
@@ -1,12 +1,10 @@
 from BaseController import BaseController
-import tornado.ioloop
-import tornado.web
 import dateutil.parser
 import datetime
-
+from twisted.internet import defer
 
 class MemoryController(BaseController):
-
+    @defer.inlineCallbacks
     def get(self):
         server = self.get_argument("server")
         from_date = self.get_argument("from", None)
@@ -28,8 +26,9 @@ class MemoryController(BaseController):
         prev_max=0
         prev_current=0
         counter=0
-
-        for data in self.stats_provider.get_memory_info(server, start, end):
+        mi = yield self.stats_provider.get_memory_info(server, start, end)
+        
+        for data in mi:
             combined_data.append([data[0], data[1], data[2]])
 
         for data in combined_data:
diff --git a/src/api/controller/TopCommandsController.py b/src/api/controller/TopCommandsController.py
index cb7b67b..3dd4f51 100644
--- a/src/api/controller/TopCommandsController.py
+++ b/src/api/controller/TopCommandsController.py
@@ -1,12 +1,11 @@
 from BaseController import BaseController
-import tornado.ioloop
-import tornado.web
 import dateutil.parser
 import datetime
+from twisted.internet import defer
 
 
 class TopCommandsController(BaseController):
-
+    @defer.inlineCallbacks
     def get(self):
         return_data = dict(data=[],
                            timestamp=datetime.datetime.now().isoformat())
@@ -22,9 +21,10 @@ class TopCommandsController(BaseController):
         else:
             start = dateutil.parser.parse(from_date)
             end   = dateutil.parser.parse(to_date)
+        
+        tc = yield self.stats_provider.get_top_commands_stats(server, start, end)
 
-        for data in self.stats_provider.get_top_commands_stats(server, start,
-                                                               end):
+        for data in tc:
             return_data['data'].append([data[0], data[1]])
 
         self.write(return_data)
diff --git a/src/api/controller/TopKeysController.py b/src/api/controller/TopKeysController.py
index 9edaaa8..6398537 100644
--- a/src/api/controller/TopKeysController.py
+++ b/src/api/controller/TopKeysController.py
@@ -1,12 +1,11 @@
 from BaseController import BaseController
-import tornado.ioloop
-import tornado.web
 import dateutil.parser
 import datetime
-
+from twisted.internet import defer
 
 class TopKeysController(BaseController):
 
+  @defer.inlineCallbacks
   def get(self):
       return_data = dict(data=[], timestamp=datetime.datetime.now().isoformat())
 
@@ -22,7 +21,9 @@ class TopKeysController(BaseController):
           start = dateutil.parser.parse(from_date)
           end   = dateutil.parser.parse(to_date)
 
-      for data in self.stats_provider.get_top_keys_stats(server, start, end):
+      tk = yield self.stats_provider.get_top_keys_stats(server, start, end)
+
+      for data in tk:
           return_data['data'].append([data[0], data[1]])
 
       self.write(return_data)
diff --git a/src/dataprovider/dataprovider.py b/src/dataprovider/dataprovider.py
index 98e955c..7a53a7c 100644
--- a/src/dataprovider/dataprovider.py
+++ b/src/dataprovider/dataprovider.py
@@ -1,6 +1,7 @@
 from api.util import settings
 import sqliteprovider
 import redisprovider
+import txredisprovider
 
 
 # TODO: Confirm there's not some implementation detail I've missed, then
@@ -8,15 +9,18 @@ import redisprovider
 class RedisLiveDataProvider(object):
 
     @staticmethod
-    def get_provider():
+    def get_provider(data_store_type = None):
         """Returns a data provider based on the settings file.
 
-        Valid providers are currently Redis and SQLite.
+        Valid providers are currently Blocking/Tornado Redis, txRedisAPI based for twitsted and cyclone and SQLite.
         """
-        data_store_type = settings.get_data_store_type()
+        if (data_store_type == None): 
+            data_store_type = settings.get_data_store_type()
 
         # FIXME: Should use a global variable for "redis" here.
         if data_store_type == "redis":
             return redisprovider.RedisStatsProvider()
+        elif data_store_type == "txredis":
+            return txredisprovider.TxRedisStatsProvider()
         else:
             return sqliteprovider.RedisStatsProvider()
diff --git a/src/dataprovider/txredisprovider.py b/src/dataprovider/txredisprovider.py
new file mode 100644
index 0000000..2bbf677
--- /dev/null
+++ b/src/dataprovider/txredisprovider.py
@@ -0,0 +1,330 @@
+from api.util import settings
+from datetime import datetime, timedelta
+import cyclone.redis
+import json
+import ast
+from twisted.internet import defer
+
+class TxRedisStatsProvider(object):
+    """A Redis based persistance to store and fetch stats"""
+
+    def __init__(self):
+        # redis server to use to store stats
+        stats_server = settings.get_redis_stats_server()
+        self.server = stats_server["server"]
+        self.port = stats_server["port"]
+        self.dbid = 0
+        self.poolsize = 10
+        self.conn = cyclone.redis.lazyConnectionPool(self.server, self.port, self.dbid, self.poolsize)
+
+    @defer.inlineCallbacks
+    def save_memory_info(self, server, timestamp, used, peak):
+        """Saves used and peak memory stats,
+
+        Args:
+            server (str): The server ID
+            timestamp (datetime): The time of the info.
+            used (int): Used memory value.
+            peak (int): Peak memory value.
+        """
+        data = {"timestamp": timestamp.strftime('%s'),
+                "used": used,
+                "peak": peak}
+        yield self.conn.zadd(server + ":memory", timestamp.strftime('%s'), data)
+
+    @defer.inlineCallbacks
+    def save_info_command(self, server, timestamp, info):
+        """Save Redis info command dump
+
+        Args:
+            server (str): id of server
+            timestamp (datetime): Timestamp.
+            info (dict): The result of a Redis INFO command.
+        """
+        yield self.conn.set(server + ":Info", json.dumps(info))
+
+    @defer.inlineCallbacks
+    def save_monitor_command(self, server, timestamp, command, keyname,
+                             argument):
+        """save information about every command
+
+        Args:
+            server (str): Server ID
+            timestamp (datetime): Timestamp.
+            command (str): The Redis command used.
+            keyname (str): The key the command acted on.
+            argument (str): The args sent to the command.
+        """
+
+        epoch = timestamp.strftime('%s')
+        current_date = timestamp.strftime('%y%m%d')
+
+        # start a redis MULTI/EXEC transaction
+        pipeline = yield self.conn.multi()
+
+        # store top command and key counts in sorted set for every second
+        # top N are easily available from sorted set in redis
+        # also keep a sorted set for every day
+        # switch to daily stats when stats requsted are for a longer time period        
+
+        command_count_key = server + ":CommandCount:" + epoch
+        yield pipeline.zincrby(command_count_key, command, 1)
+
+        command_count_key = server + ":DailyCommandCount:" + current_date
+        yield pipeline.zincrby(command_count_key, command, 1)
+
+        key_count_key = server + ":KeyCount:" + epoch
+        yield pipeline.zincrby(key_count_key, keyname, 1)
+
+        key_count_key = server + ":DailyKeyCount:" + current_date
+        yield pipeline.zincrby(key_count_key, command, 1)
+
+        # keep aggregate command in a hash
+        command_count_key = server + ":CommandCountBySecond"
+        yield pipeline.hincrby(command_count_key, epoch, 1)
+
+        command_count_key = server + ":CommandCountByMinute"
+        field_name = current_date + ":" + str(timestamp.hour) + ":"
+        field_name += str(timestamp.minute)
+        yield pipeline.hincrby(command_count_key, field_name, 1)
+
+        command_count_key = server + ":CommandCountByHour"
+        field_name = current_date + ":" + str(timestamp.hour)
+        yield pipeline.hincrby(command_count_key, field_name, 1)
+
+        command_count_key = server + ":CommandCountByDay"
+        field_name = current_date
+        yield pipeline.hincrby(command_count_key, field_name, 1)
+
+        # commit transaction to redis
+        yield pipeline.commit()
+
+    @defer.inlineCallbacks
+    def get_info(self, server):
+        """Get info about the server
+
+        Args:
+            server (str): The server ID
+        """
+        info = yield self.conn.get(server + ":Info")
+        # FIXME: If the collector has never been run we get a 500 here. `None`
+        # is not a valid type to pass to json.loads.
+        info = json.loads(info)
+        defer.returnValue(info)
+
+    @defer.inlineCallbacks
+    def get_memory_info(self, server, from_date, to_date):
+        """Get stats for Memory Consumption between a range of dates
+
+        Args:
+            server (str): The server ID
+            from_date (datetime): Get memory info from this date onwards.
+            to_date (datetime): Get memory info up to this date.
+        """
+        memory_data = []
+        start = int(from_date.strftime("%s"))
+        end = int(to_date.strftime("%s"))
+        rows = yield self.conn.zrangebyscore(server + ":memory", start, end)
+
+        for row in rows:
+            # TODO: Check to see if there's not a better way to do this. Using
+            # eval feels like it could be wrong/dangerous... but that's just a
+            # feeling.
+            row = ast.literal_eval(row) #TODO
+            parts = []
+
+            # convert the timestamp
+            timestamp = datetime.fromtimestamp(int(row['timestamp']))
+            timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')
+
+            memory_data.append([timestamp, row['peak'], row['used']])
+
+        defer.returnValue(memory_data)
+
+    @defer.inlineCallbacks
+    def get_command_stats(self, server, from_date, to_date, group_by):
+        """Get total commands processed in the given time period
+
+        Args:
+            server (str): The server ID
+            from_date (datetime): Get data from this date.
+            to_date (datetime): Get data to this date.
+            group_by (str): How to group the stats.
+        """
+        s = []
+        time_stamps = []
+        key_name = ""
+
+        if group_by == "day":
+            key_name = server + ":CommandCountByDay"
+            t = from_date.date()
+            while t <= to_date.date():
+                s.append(t.strftime('%y%m%d'))
+                time_stamps.append(t.strftime('%s'))
+                t = t + timedelta(days=1)
+
+        elif group_by == "hour":
+            key_name = server + ":CommandCountByHour"
+
+            t = from_date
+            while t<= to_date:
+                field_name = t.strftime('%y%m%d') + ":" + str(t.hour)
+                s.append(field_name)
+                time_stamps.append(t.strftime('%s'))
+                t = t + timedelta(seconds=3600)
+
+        elif group_by == "minute":
+            key_name = server + ":CommandCountByMinute"
+
+            t = from_date
+            while t <= to_date:
+                field_name = t.strftime('%y%m%d') + ":" + str(t.hour)
+                field_name += ":" + str(t.minute)
+                s.append(field_name)
+                time_stamps.append(t.strftime('%s'))
+                t = t + timedelta(seconds=60)
+
+        else:
+            key_name = server + ":CommandCountBySecond"
+            start = int(from_date.strftime("%s"))
+            end = int(to_date.strftime("%s"))
+            for x in range(start, end + 1):
+                s.append(str(x))
+                time_stamps.append(x)
+
+        data = []
+        counts = yield self.conn.hmget(key_name, s)
+        for x in xrange(0,len(counts)):
+            # the default time format string
+            time_fmt = '%Y-%m-%d %H:%M:%S'
+
+            if group_by == "day":
+                time_fmt = '%Y-%m-%d'
+            elif group_by == "hour":
+                time_fmt = '%Y-%m-%d %H:00:00'
+            elif group_by == "minute":
+                time_fmt = '%Y-%m-%d %H:%M:00'
+
+            # get the count.
+            try:
+                if counts[x] is not None: 
+                    count = int(counts[x])
+                else:
+                    count = 0
+            except Exception as e:
+                count = 0
+
+            # convert the timestamp
+            timestamp = int(time_stamps[x])
+            timestamp = datetime.fromtimestamp(timestamp)
+            timestamp = timestamp.strftime(time_fmt)
+
+            # add to the data
+            data.append([count, timestamp])
+        defer.returnValue(reversed(data))
+
+    @defer.inlineCallbacks
+    def get_top_commands_stats(self, server, from_date, to_date):
+        """Get top commands processed in the given time period
+
+        Args:
+            server (str): Server ID
+            from_date (datetime): Get stats from this date.
+            to_date (datetime): Get stats to this date.
+        """
+
+        counts = self.get_top_counts(server, from_date, to_date, "CommandCount",
+                                     "DailyCommandCount")
+        defer.returnValue(reversed(counts))
+
+    @defer.inlineCallbacks
+    def get_top_keys_stats(self, server, from_date, to_date):
+        """Gets top comm processed
+
+        Args:
+            server (str): Server ID
+            from_date (datetime): Get stats from this date.
+            to_date (datetime): Get stats to this date.
+        """
+        defer.returnValue(self.get_top_counts(server, from_date, to_date, "KeyCount",
+                                   "DailyKeyCount"))
+
+
+    # Helper methods
+
+    @defer.inlineCallbacks
+    def get_top_counts(self, server, from_date, to_date, seconds_key_name,
+                       day_key_name, result_count=None):
+        """Top counts are stored in a sorted set for every second and for every
+        day. ZUNIONSTORE across the timeperiods generates the results.
+
+        Args:
+            server (str): The server ID
+            from_date (datetime): Get stats from this date.
+            to_date (datetime): Get stats to this date.
+            seconds_key_name (str): The key for stats at second resolution.
+            day_key_name (str): The key for stats at daily resolution.
+
+        Kwargs:
+            result_count (int): The number of results to return. Default: 10
+        """
+        if result_count is None:
+            result_count = 10
+
+        # get epoch
+        start = int(from_date.strftime("%s"))
+        end = int(to_date.strftime("%s"))
+        diff = to_date - from_date
+
+        # start a redis MULTI/EXEC transaction
+        pipeline = yield self.conn.multi()
+
+        # store the set names to use in ZUNIONSTORE in a list
+        s = []
+
+        if diff.days > 2 :
+            # when difference is over 2 days, no need to check counts for every second
+            # Calculate:
+            # counts of every second on the start day
+            # counts of every day in between
+            # counts of every second on the end day
+            next_day = from_date.date() + timedelta(days=1)
+            prev_day = to_date.date() - timedelta(days=1)
+            from_date_end_epoch = int(next_day.strftime("%s")) - 1
+            to_date_begin_epoch = int(to_date.date().strftime("%s"))
+
+            # add counts of every second on the start day
+            for x in range(start, from_date_end_epoch + 1):
+                s.append(":".join([server, seconds_key_name, str(x)]))
+
+            # add counts of all days in between
+            t = next_day
+            while t <= prev_day:
+                s.append(":".join([server, day_key_name, t.strftime('%y%m%d')]))
+                t = t + timedelta(days=1)
+
+            # add counts of every second on the end day
+            for x in range(to_date_begin_epoch, end + 1):
+                s.append(server + ":" + seconds_key_name + ":" + str(x))
+
+        else:
+            # add counts of all seconds between start and end date
+            for x in range(start, end + 1):
+                s.append(server + ":" + seconds_key_name + ":" + str(x))
+
+        # store the union of all the sets in a temp set
+        temp_key_name = "_top_counts"
+        yield pipeline.zunionstore(temp_key_name, s)
+        yield pipeline.zrange(temp_key_name, 0, result_count - 1, True)
+        yield pipeline.delete(temp_key_name)
+
+        # commit transaction to redis
+        results = yield pipeline.commit()
+        print results
+        result_data = []
+        l = results[-2]
+        res = [(l[n], l[n+1]) for n in range(0, len(l), 2)]
+        for val, count in res:
+            result_data.append([val, count])
+
+        defer.returnValue(result_data)
diff --git a/src/redis-live.conf b/src/redis-live.conf
index 8b25ff8..8da90f0 100644
--- a/src/redis-live.conf
+++ b/src/redis-live.conf
@@ -12,6 +12,6 @@
 	"RedisStatsServer":
 	{
 		"server" : "127.0.0.1",
-		"port" : 6381
+		"port" : 6379
 	}	
 }
diff --git a/src/redis-live.py b/src/redis-live.py
index 43479f4..9318c35 100755
--- a/src/redis-live.py
+++ b/src/redis-live.py
@@ -1,8 +1,8 @@
 #! /usr/bin/env python
 
-import tornado.ioloop
-import tornado.options
-import tornado.web
+from twisted.internet import reactor
+import cyclone.options
+import cyclone.web
 
 from api.controller.BaseStaticFileHandler import BaseStaticFileHandler
 
@@ -15,7 +15,7 @@ from api.controller.TopKeysController import TopKeysController
 
 
 # Bootup
-application = tornado.web.Application([
+application = cyclone.web.Application([
   (r"/api/servers", ServerListController),
   (r"/api/info", InfoController),
   (r"/api/memory", MemoryController),
@@ -27,6 +27,6 @@ application = tornado.web.Application([
 
 
 if __name__ == "__main__":
-	tornado.options.parse_command_line()
-	application.listen(8888)
-	tornado.ioloop.IOLoop.instance().start()
+  cyclone.options.parse_command_line()
+  reactor.listenTCP(8888, application, interface="127.0.0.1")                 
+  reactor.run()
